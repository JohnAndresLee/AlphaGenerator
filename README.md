### Q1: Alphagen Framework Overview

The paper titled **"Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning"** presents a novel framework for discovering and combining alpha factors in quantitative trading using reinforcement learning. Alpha factors are signals derived from historical stock data that help predict market trends. The authors aim to find a set of formulaic alphas that work synergistically to improve predictive accuracy, unlike traditional methods that mine alphas individually without considering their combined effects.

#### Key Contributions:

1. **Synergistic Alpha Mining**: The proposed method focuses on generating a set of alpha factors that perform well together, rather than evaluating them in isolation. This is crucial because in practice, alpha factors are often used in combination to model complex stock market behaviors.

2. **Reinforcement Learning Approach**: The framework leverages RL to explore the vast space of possible formulaic alphas. The performance improvement of a combination model is used as the reward signal to guide the RL-based alpha generator. This contrasts with traditional genetic programming (GP)-based methods, which can struggle with large search spaces.

3. **Optimization Objective**: The performance of the combined alpha factors is optimized directly in the RL process, rather than relying on indirect measures like mutual information or information coefficients (IC) between individual alphas, which can miss the potential synergies between factors.

4. **Experimental Results**: The framework is tested on real-world stock market data, demonstrating superior performance in both prediction accuracy and investment simulation compared to traditional approaches. The method achieves higher returns in backtesting, showing practical applicability for stock trend forecasting.

#### Key Findings:

- The proposed framework outperforms traditional methods like genetic programming and other RL-based approaches that optimize single alpha performance. It can continuously find synergistic alpha sets even as the pool size grows.

- The experiments show that combining multiple well-performing alphas, which individually might not be diverse (as measured by mutual IC), can result in better overall predictive performance due to the synergy between the factors.

### Q2-a: Data Download from binance(download_data.py)

The primary objective of the script is to:

1. Retrieve Binance kline data for 15-minute (15m) and 1-hour (1h) intervals.

2. Ensure that data is fetched for the past two years, regardless of the current date.

3. Store the data in a format that can be easily analyzed for qlib.

The Binance API requires the following parameters to fetch kline data:

- **symbol**: The trading pair (e.g., BTCUSDT).

- **interval**: The time interval (e.g., 15m, 1h).

- **start_time**: The time from which to start fetching data.

- **end_time**: The end time for fetching the data.

request sample:

```python
BASE_URL = 'https://api.binance.com'
KLINE_ENDPOINT = '/api/v3/klines'

url = BASE_URL + KLINE_ENDPOINT
params = {
    'symbol': symbol,
    'interval': interval,
    'startTime': start_ts,
    'limit': limit
}
response = requests.get(url, params=params)
```

### Q2-b: Generate Alpha with Alphagen(main.py)

The `main.py` script is based on the `train_maskable_ppo.py` file in alphagen, with the main parameter modifications including: 

1. The time range for the training/validation/test sets

   <img src="https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922175146902.png" alt="image-20240922175146902" style="zoom:50%;" />

2. Changing the target to the next tick's return

3. Updating the instrument to `15min_symbols`

During the process of writing the `main.py` code, I gained a deeper understanding of the data and model framework of Alphagen and Qlib. By modifying the original training code, which was designed for daily frequency data, to support 15-minute frequency, I expanded the flexibility and scalability of the initial model.

#### Test Result

Using the factors generated in the last JSON file as sample results from the model, there are a total of 20 factors. The correlations between the factors are shown in the figure below, and overall, the correlations are relatively low.

![image-20240922231927077](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922231927077.png)

IC result:

<img src="https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922232239055.png" alt="image-20240922232239055"  />

The low IC of factors generated by reinforcement learning can be attributed to several reasons:

1. **Large Search Space**:
   In financial factor mining, the search space is vast. Although RL has strong exploration capabilities, it might struggle to efficiently identify the optimal factors in such a large space, especially in uncertain financial environments. RL could explore many low-quality factors, leading to an overall lower IC.
2. **Training Convergence Challenges**:
   RL algorithms often require significant time to converge and are sensitive to hyperparameters. In financial factor generation, RL may not have sufficient iterations, or its hyperparameters may not be tuned optimally, causing the model to fail in discovering high-IC factors.
3. **Complexity of Financial Markets**:
   Financial markets are inherently complex and noisy. Even with powerful algorithms like RL, the model can be influenced by random market fluctuations, resulting in factors that lack stable predictive power. While RL can generate complex factors, their effectiveness (IC) might be reduced due to market noise and overfitting.
4. **Insufficient or Noisy Data**:
   If the training data is insufficient or noisy, the RL model may struggle to learn robust factors. Financial markets are highly noisy, especially with high-frequency data (e.g., 15-minute intervals), which can lead to lower IC values.

### Q3-a: Generate Alpha with Baseline Methods

#### Part1: GP-based methods

#### Part2: Deep Symbolic Regression method

Generated over 6000 valid factors

![image-20240922224943616](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922224943616.png)

Top 20 factors by IC value

![image-20240922233657977](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922233657977.png)

Top 20 factors' correlation

![image-20240922234723991](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240922234723991.png)

The high correlation between the top 20 factors generated by a genetic programming model can be explained by several factors:

1. **Similar Structure of Expression Trees**:
   Genetic programming works by evolving expression trees to create factors. Over time, the evolutionary process may favor certain types of tree structures or mathematical operators that consistently yield better results. As a result, many of the top-performing factors might share similar structural elements, leading to high correlation between them.

2. **Overfitting to Historical Data**:
   GP models optimize factors based on historical data. If the optimization process strongly overfits to specific patterns in the data, it can result in factors that capture the same market behavior or trend. Even though they appear different, these factors may react similarly to certain market conditions, increasing their correlation.

3. **Limited Diversity in Features**:
   The factors generated by GP are typically based on a limited set of underlying features (e.g., price, volume, moving averages). If the model is using the same features repeatedly in different combinations, the resulting factors may end up being highly correlated because they are ultimately based on the same input data.

4. **Selection Pressure**:
   In genetic programming, high-performing factors are more likely to be selected for reproduction and mutation. If certain mathematical forms or combinations of features consistently perform well, they will dominate the population, leading to many similar factors being generated and retained in the top 20.

5. **Narrow Search Space**:
   GP may explore only a narrow subset of the potential factor space due to limitations in mutation and crossover operations. If the search space is not broad enough, the algorithm may converge on a set of similar solutions that perform well, but are not sufficiently diverse.

### Q4-a: Rank, Compare and Filter Generated Alpha(alpha_ranking.py)

Breakdown of what each function does:

1. load_alphas(): Loads CSV files containing alpha data from a specified folder into a DataFrame.

2. rank_alphas(): Ranks alphas based on their correlation with a target variable, filtering out weak correlations.

3. cal_corr(): Calculates the correlation matrix of the remaining alphas, identifying and removing highly correlated factors (above a defined threshold).

4. save_unique_alphas(): Saves the filtered, unique alphas back to a new folder.

5. main(): Ties all the steps together—loading, ranking, filtering, and saving unique alphas.

**The detailed running result of alpha ranking function is presented in Q2-b step3.**

### Q4-b: Alpha Backtest

Selected alphas backtest result

- **Mul(Greater(Constant(10.0),Mad(Min(Max($close,10),20),10)),$volume)**: Generated by RL model

Daily IC & Cumulative IC Plot:

![image-20240923110147346](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240923110147346.png)

Cumulative Return in Groups:

<img src="https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240923113602216.png" alt="image-20240923113602216" style="zoom:50%;" />

- **Corr($open,$low,30)**: Generated by GP model

Daily IC & Cumulative IC Plot:

![image-20240923085238597](https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240923085238597.png)

Cumulative Return in Groups:

<img src="https://raw.githubusercontent.com/JohnAndresLee/websitepicture/main/image-20240923105410033.png" alt="image-20240923105410033" style="zoom: 50%;" />

From the above result, I find that although Reinforcement learning generated factors may exhibit lower IC values compared to those produced by genetic programming, but they **tend to be more stable** for several reasons:

1. **Exploration and Generalization**: RL focuses on exploration across the entire solution space, which can result in discovering more robust, generalizable factors that perform steadily, even if their IC is lower.

2. **Reward-Based Optimization**: RL directly optimizes factor performance using a reward function, aligning better with the ultimate objective, leading to more consistent results across different market conditions.

3. **Noise Management**: RL’s iterative approach helps it adapt to noisy environments, which enhances the stability of the factors compared to GP, which may overfit to specific market conditions during evolution. This adaptability results in more reliable, though slightly less aggressive, factors.

### Q5: Conclusion

In this project, I familiarized and further developed the alphagen framework to generate alpha factors for crypto market. I began by researching methods such as genetic programming and reinforcement learning to generate alpha factors, understanding the strengths and weaknesses of each. I then learned to calculate Information Coefficient and tested alpha performance using techniques like simple long-short strategies. I created Python scripts to load, rank, and backtest alpha factors, while visualizing results with cumulative returns and correlation heatmaps. This involved integrating `pandas`, `matplotlib`, and other libraries for effective analysis.

Through this process, I deepened my understanding of financial factor research and backtesting, learning how to combine theory with practical coding to analyze complex data in an efficient and structured way.